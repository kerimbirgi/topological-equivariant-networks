# @package _global_
model:
  num_hidden: 128

training:
  batch_size: 256
  weight_decay: 1e-16
  scheduler_mode: "cosine_annealing"
  min_lr: 0.0
  num_lr_cycles: 3
  clip_gradients: true
  clip_amount: 1.0
  lr: 1e-3
  crit: 'L1Loss'